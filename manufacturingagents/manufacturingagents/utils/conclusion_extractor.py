# ç»“è®ºæå–æ™ºèƒ½ä½“
# Conclusion Extractor for Manufacturing

from langchain_core.messages import AIMessage
import time
import json
from manufacturingagents.manufacturingagents.prompts.prompt_manager import prompt_manager


def create_conclusion_extractor(llm, memory):
    """åˆ›å»ºç»“è®ºæå–æ™ºèƒ½ä½“"""
    
    def conclusion_extractor_node(state):
        print(f"ğŸ“‹ [DEBUG] ===== ç»“è®ºæå–æ™ºèƒ½ä½“èŠ‚ç‚¹å¼€å§‹ =====")
        
        # ğŸ¯ è·å–è¿›åº¦è¿½è¸ªå™¨å¹¶è®°å½•é˜¶æ®µ
        progress_callback = state.get('progress_callback')
        if progress_callback:
            progress_callback.update_progress(7)
            progress_callback.log_decision_phase("ç»“è®ºæå–æ™ºèƒ½ä½“ç”Ÿæˆç»“æ„åŒ–è¾“å‡º")
        
        current_date = state["analysis_date"]
        product_type = state["product_type"]
        company_name = state["company_name"]
        
        print(f"ğŸ“‹ [DEBUG] è¾“å…¥å‚æ•°: product_type={product_type}, company={company_name}, date={current_date}")
        
        # è·å–å†³ç­–åè°ƒæŠ¥å‘Šå’Œé£é™©è¯„ä¼°æŠ¥å‘Š
        decision_coordination_plan = state.get("decision_coordination_plan", "")
        risk_assessment_report = state.get("risk_assessment_report", "")
        
        print(f"ğŸ“‹ [DEBUG] å†³ç­–åè°ƒæŠ¥å‘Šé•¿åº¦: {len(decision_coordination_plan)}")
        print(f"ğŸ“‹ [DEBUG] é£é™©è¯„ä¼°æŠ¥å‘Šé•¿åº¦: {len(risk_assessment_report)}")
        
        # ğŸ¯ ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–åŸºç¡€æç¤ºè¯
        base_system_prompt = prompt_manager.get_prompt("conclusion_extractor")
        if not base_system_prompt:
            # é™çº§å¤„ç†ï¼šå¦‚æœæç¤ºè¯æ–‡ä»¶ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            base_system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç»“è®ºæå–ä¸“å®¶ï¼Œè´Ÿè´£ä»åˆ†ææŠ¥å‘Šä¸­æå–å…³é”®ç»“è®ºå¹¶è¾“å‡ºJSONæ ¼å¼æ•°æ®ã€‚"
        
        # ç»“åˆå…·ä½“ä»»åŠ¡ä¿¡æ¯æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
        system_message = f"""{base_system_prompt}

ğŸ¯ å½“å‰æå–ä»»åŠ¡ï¼š
- åˆ†æäº§å“ç±»å‹: {product_type}
- åˆ†æå…¬å¸: {company_name}
- åˆ†ææ—¥æœŸ: {current_date}

ğŸ“Š éœ€è¦æå–ç»“è®ºçš„æŠ¥å‘Šå†…å®¹ï¼š

### å†³ç­–åè°ƒå‘˜æŠ¥å‘Š
{decision_coordination_plan}

### é£é™©è¯„ä¼°æŠ¥å‘Š
{risk_assessment_report}

ğŸ“‹ ç‰¹åˆ«è¦æ±‚ï¼š
- ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸å¾—åŒ…å«å…¶ä»–æ–‡å­—
- ç¡®ä¿ç­–ç•¥ã€å¹…åº¦ã€é£é™©é€»è¾‘ä¸€è‡´
- æ‰€æœ‰æ•°å€¼å¿…é¡»å‡†ç¡®åæ˜ æŠ¥å‘Šå†…å®¹
- emojiä½¿ç”¨å¿…é¡»ä¸ç­–ç•¥åŒ¹é…

ç°åœ¨è¯·åŸºäºä¸Šè¿°æŠ¥å‘Šå†…å®¹ï¼Œæå–å…³é”®ç»“è®ºå¹¶è¾“å‡ºæ ‡å‡†JSONæ ¼å¼ï¼"""
        
        # è°ƒç”¨LLM
        response = llm.invoke(system_message)
        
        # æ ¼å¼åŒ–å›å¤ - å…¼å®¹ä¸åŒLLMå“åº”æ ¼å¼
        if hasattr(response, 'content'):
            content = response.content
        elif isinstance(response, str):
            content = response
        else:
            content = str(response)
        
        print(f"ğŸ“‹ [DEBUG] LLMåŸå§‹è¾“å‡º: {content[:200]}...")
        
        # ğŸ¯ å°è¯•æå–å’ŒéªŒè¯JSON
        try:
            # æå–JSONå†…å®¹ï¼ˆå¤„ç†å¯èƒ½çš„markdownæ ¼å¼ï¼‰
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_content = content[json_start:json_end]
                
                # éªŒè¯JSONæ ¼å¼
                conclusion_data = json.loads(json_content)
                
                print(f"ğŸ“‹ [DEBUG] JSONæå–æˆåŠŸ: {json.dumps(conclusion_data, ensure_ascii=False, indent=2)}")
                
                # ä¿å­˜ç»“æ„åŒ–ç»“è®º
                state["conclusion_json"] = conclusion_data
                state["conclusion_raw"] = content
                
            else:
                print(f"ğŸ“‹ [WARNING] æ— æ³•æå–æœ‰æ•ˆJSONï¼Œä½¿ç”¨åŸå§‹è¾“å‡º")
                state["conclusion_raw"] = content
                state["conclusion_json"] = None
                
        except json.JSONDecodeError as e:
            print(f"ğŸ“‹ [ERROR] JSONè§£æå¤±è´¥: {e}")
            state["conclusion_raw"] = content
            state["conclusion_json"] = None
        
        # æ›´æ–°æ¶ˆæ¯
        state["messages"].append(AIMessage(content=content))
        
        print(f"ğŸ“‹ [DEBUG] ç»“è®ºæå–å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(content)}")
        
        return state
    
    return conclusion_extractor_node 