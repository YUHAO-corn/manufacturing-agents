# å†³ç­–åè°ƒå‘˜
# Decision Coordinator for Manufacturing

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage
import time
import json
from manufacturingagents.manufacturingagents.prompts.prompt_manager import prompt_manager


def create_decision_coordinator(llm, memory):
    """åˆ›å»ºå†³ç­–åè°ƒå‘˜"""
    
    def decision_coordinator_node(state):
        print(f"âš–ï¸ [DEBUG] ===== å†³ç­–åè°ƒå‘˜èŠ‚ç‚¹å¼€å§‹ =====")
        
        # ğŸ¯ æ–°å¢ï¼šè·å–è¿›åº¦è¿½è¸ªå™¨å¹¶è®°å½•å†³ç­–é˜¶æ®µ
        progress_callback = state.get('progress_callback')
        if progress_callback:
            progress_callback.update_progress(6)
            progress_callback.log_decision_phase("å†³ç­–åè°ƒå‘˜ç»¼åˆåˆ†æ")
        
        current_date = state["analysis_date"]
        product_type = state["product_type"]
        company_name = state["company_name"]
        
        print(f"âš–ï¸ [DEBUG] è¾“å…¥å‚æ•°: product_type={product_type}, company={company_name}, date={current_date}")
        
        # ä»æç¤ºè¯ç®¡ç†å™¨è·å–ç³»ç»Ÿæç¤ºè¯
        base_system_prompt = prompt_manager.get_prompt("decision_coordinator")
        if not base_system_prompt:
            # å¦‚æœæç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯
            base_system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åˆ¶é€ ä¸šè¡¥è´§å†³ç­–åè°ƒå‘˜ï¼Œè´Ÿè´£æ•´åˆå„æ–¹åˆ†æå’Œå»ºè®®ï¼Œåè°ƒå†³ç­–è¿‡ç¨‹ã€‚"
        
        # ç»“åˆå…·ä½“ä»»åŠ¡ä¿¡æ¯æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
        system_message = f"""{base_system_prompt}

ğŸ¯ å½“å‰åè°ƒä»»åŠ¡ï¼š
- åˆ†æäº§å“ç±»å‹: {product_type}
- åˆ†æå…¬å¸: {company_name}
- åˆ†ææ—¥æœŸ: {current_date}

ğŸ“Š éœ€è¦æ•´åˆçš„ä¿¡æ¯ï¼š
- å¸‚åœºç¯å¢ƒåˆ†ææŠ¥å‘Š
- è¶‹åŠ¿é¢„æµ‹åˆ†ææŠ¥å‘Š
- è¡Œä¸šèµ„è®¯åˆ†ææŠ¥å‘Š
- æ¶ˆè´¹è€…æ´å¯Ÿåˆ†ææŠ¥å‘Š
- ä¹è§‚å†³ç­–é¡¾é—®å»ºè®®
- è°¨æ…å†³ç­–é¡¾é—®å»ºè®®

ğŸ“‹ ç‰¹åˆ«è¦æ±‚ï¼š
- ç»¼åˆè€ƒè™‘æ‰€æœ‰åˆ†æç»“æœ
- å¹³è¡¡æœºä¼šä¸é£é™©
- æä¾›æ˜ç¡®çš„å†³ç­–å»ºè®®
- åŒ…å«æ‰§è¡ŒæŒ‡å¯¼å’Œåº”æ€¥é¢„æ¡ˆ
- æŠ¥å‘Šé•¿åº¦ä¸å°‘äº1000å­—

è¯·åŸºäºæ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œæä¾›æœ€ç»ˆçš„è¡¥è´§å†³ç­–åè°ƒæŠ¥å‘Šã€‚"""

        # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="messages"),
        ])
        
        # åˆ›å»ºé“¾
        chain = prompt | llm
        
        print(f"âš–ï¸ [DEBUG] è°ƒç”¨LLMé“¾...")
        result = chain.invoke(state["messages"])
        
        print(f"âš–ï¸ [DEBUG] LLMè°ƒç”¨å®Œæˆ")
        
        # å…¼å®¹ä¸åŒLLMå“åº”æ ¼å¼
        if hasattr(result, 'content'):
            llm_content = result.content
        elif isinstance(result, str):
            llm_content = result
        else:
            llm_content = str(result)
        
        # ğŸ¯ ä¿®å¤ï¼šä½¿ç”¨LLMçš„çœŸå®è¾“å‡ºä½œä¸ºå†³ç­–åè°ƒæŠ¥å‘Š
        decision_coordination_report = llm_content
        
        # æ›´æ–°çŠ¶æ€
        state["decision_coordination_plan"] = decision_coordination_report
        state["messages"].append(AIMessage(content=decision_coordination_report))
        
        print(f"âš–ï¸ [DEBUG] å†³ç­–åè°ƒå®Œæˆï¼ŒæŠ¥å‘Šé•¿åº¦: {len(decision_coordination_report)}")
        
        return state
    
    return decision_coordinator_node 