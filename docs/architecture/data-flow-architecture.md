# åˆ¶é€ ä¸šæ•°æ®æµæ¶æ„

## ğŸ¯ æ•°æ®æµæ¦‚è¿°

åˆ¶é€ ä¸šæ™ºèƒ½è¡¥è´§å†³ç­–ç³»ç»Ÿçš„æ•°æ®æµæ¶æ„è®¾è®¡äº†ä»å¤–éƒ¨æ•°æ®æºåˆ°æœ€ç»ˆå†³ç­–è¾“å‡ºçš„å®Œæ•´æ•°æ®å¤„ç†é“¾è·¯ï¼Œç¡®ä¿æ•°æ®çš„å®æ—¶æ€§ã€å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

### æ ¸å¿ƒè®¾è®¡åŸåˆ™
- **ğŸ”„ å¤šæºèåˆ**: é›†æˆå¤šä¸ªå¤–éƒ¨æ•°æ®æºï¼Œæä¾›å…¨é¢ä¿¡æ¯è§†è§’
- **âš¡ å®æ—¶å¤„ç†**: æ”¯æŒå®æ—¶æ•°æ®è·å–å’Œå¤„ç†ï¼Œå¿«é€Ÿå“åº”å¸‚åœºå˜åŒ–
- **ğŸ›¡ï¸ å®¹é”™è®¾è®¡**: å¤šå±‚é™çº§æœºåˆ¶ï¼Œç¡®ä¿åœ¨æ•°æ®æºæ•…éšœæ—¶ç³»ç»Ÿç¨³å®šè¿è¡Œ
- **ğŸ“Š æ™ºèƒ½ç¼“å­˜**: ä¸‰å±‚ç¼“å­˜æ¶æ„ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•°æ®æ–°é²œåº¦

## ğŸ—ï¸ æ•´ä½“æ•°æ®æµæ¶æ„

```mermaid
graph TD
    subgraph "ğŸŒ å¤–éƒ¨æ•°æ®æºå±‚"
        TUSHARE[TuShare Pro API<br/>å®è§‚ç»æµæ•°æ®]
        JUHE[èšåˆæ•°æ®API<br/>å¤©æ°”/èŠ‚å‡æ—¥]
        GOOGLE_NEWS[Google News API<br/>å®æ—¶æ–°é—»]
        COZE[Cozeæ’ä»¶ç¾¤<br/>ä¸“ä¸šæ•°æ®]
        DIFY[DifyçŸ¥è¯†åº“<br/>ä¸“ä¸šçŸ¥è¯†]
    end

    subgraph "ğŸ”§ æ•°æ®è·å–å±‚"
        PMI_ADAPTER[PMIæ•°æ®é€‚é…å™¨]
        PPI_ADAPTER[PPIæ•°æ®é€‚é…å™¨]
        WEATHER_ADAPTER[å¤©æ°”æ•°æ®é€‚é…å™¨]
        NEWS_ADAPTER[æ–°é—»æ•°æ®é€‚é…å™¨]
        HOLIDAY_ADAPTER[èŠ‚å‡æ—¥é€‚é…å™¨]
        COMMODITY_ADAPTER[å•†å“æœŸè´§é€‚é…å™¨]
    end

    subgraph "ğŸ§  æ™ºèƒ½ä½“å·¥å…·å±‚"
        PMI_TOOL[get_manufacturing_pmi_data]
        PPI_TOOL[get_manufacturing_ppi_data]
        WEATHER_TOOL[get_manufacturing_weather_data]
        NEWS_TOOL[get_manufacturing_news_data]
        HOLIDAY_TOOL[get_manufacturing_holiday_data]
        COMMODITY_TOOL[get_manufacturing_commodity_data]
    end

    subgraph "ğŸ’¾ ç¼“å­˜å­˜å‚¨å±‚"
        L1_CACHE[L1: Rediså†…å­˜ç¼“å­˜<br/>1å°æ—¶TTL]
        L2_CACHE[L2: MongoDBæŒä¹…åŒ–<br/>7å¤©TTL]
        L3_CACHE[L3: æ–‡ä»¶ç³»ç»Ÿç¼“å­˜<br/>æ°¸ä¹…å­˜å‚¨]
    end

    subgraph "ğŸ¤– æ™ºèƒ½ä½“æ¶ˆè´¹å±‚"
        MEA[å¸‚åœºç¯å¢ƒåˆ†æå¸ˆ]
        TPA[è¶‹åŠ¿é¢„æµ‹åˆ†æå¸ˆ]
        INA[æ–°é—»èµ„è®¯åˆ†æå¸ˆ]
        CIA[æ¶ˆè´¹è€…æ´å¯Ÿåˆ†æå¸ˆ]
    end

    subgraph "ğŸ“‹ å†³ç­–å±‚æ•°æ®"
        ANALYSIS_STATE[åˆ†æçŠ¶æ€æ•°æ®]
        DEBATE_STATE[è¾©è®ºçŠ¶æ€æ•°æ®]
        DECISION_STATE[å†³ç­–çŠ¶æ€æ•°æ®]
        FINAL_STATE[æœ€ç»ˆç»“æœæ•°æ®]
    end

    %% æ•°æ®è·å–è·¯å¾„
    TUSHARE --> PMI_ADAPTER
    TUSHARE --> PPI_ADAPTER
    TUSHARE --> COMMODITY_ADAPTER
    
    JUHE --> WEATHER_ADAPTER
    JUHE --> HOLIDAY_ADAPTER
    
    GOOGLE_NEWS --> NEWS_ADAPTER
    
    %% é€‚é…å™¨åˆ°å·¥å…·
    PMI_ADAPTER --> PMI_TOOL
    PPI_ADAPTER --> PPI_TOOL
    WEATHER_ADAPTER --> WEATHER_TOOL
    NEWS_ADAPTER --> NEWS_TOOL
    HOLIDAY_ADAPTER --> HOLIDAY_TOOL
    COMMODITY_ADAPTER --> COMMODITY_TOOL
    
    %% ç¼“å­˜æµç¨‹
    PMI_TOOL --> L1_CACHE
    PPI_TOOL --> L1_CACHE
    WEATHER_TOOL --> L1_CACHE
    NEWS_TOOL --> L1_CACHE
    
    L1_CACHE --> L2_CACHE
    L2_CACHE --> L3_CACHE
    
    %% æ™ºèƒ½ä½“æ¶ˆè´¹
    L1_CACHE --> MEA
    L1_CACHE --> TPA
    L1_CACHE --> INA
    L1_CACHE --> CIA
    
    %% çŠ¶æ€æ•°æ®æµ
    MEA --> ANALYSIS_STATE
    TPA --> ANALYSIS_STATE
    INA --> ANALYSIS_STATE
    CIA --> ANALYSIS_STATE
    
    ANALYSIS_STATE --> DEBATE_STATE
    DEBATE_STATE --> DECISION_STATE
    DECISION_STATE --> FINAL_STATE
```

## ğŸ“Š æ•°æ®æºè¯¦ç»†è®¾è®¡

### 1. TuShare Pro API
**æ•°æ®ç±»å‹**: å®è§‚ç»æµæŒ‡æ ‡  
**æ›´æ–°é¢‘ç‡**: æ—¥åº¦/æœˆåº¦  
**ä¸»è¦æ•°æ®**:

| æ•°æ®é¡¹ | APIæ¥å£ | æ›´æ–°é¢‘ç‡ | ç”¨é€” |
|--------|---------|----------|------|
| PMIæŒ‡æ•° | `pro_api.eco_cal()` | æœˆåº¦ | åˆ¶é€ ä¸šæ™¯æ°”åº¦åˆ†æ |
| PPIæŒ‡æ•° | `pro_api.eco_cal()` | æœˆåº¦ | ç”Ÿäº§è€…ä»·æ ¼è¶‹åŠ¿ |
| å·¥ä¸šå¢åŠ å€¼ | `pro_api.eco_cal()` | æœˆåº¦ | å·¥ä¸šç”Ÿäº§æ°´å¹³ |
| æœŸè´§ä»·æ ¼ | `pro_api.fut_daily()` | æ—¥åº¦ | åŸææ–™æˆæœ¬é¢„æµ‹ |

```python
class TuShareDataAdapter:
    """TuShareæ•°æ®é€‚é…å™¨"""
    
    def get_pmi_data(self, start_date: str, end_date: str) -> Dict:
        """è·å–PMIæ•°æ®"""
        try:
            # è°ƒç”¨TuShare API
            df = self.pro_api.eco_cal(
                start_date=start_date,
                end_date=end_date,
                event='åˆ¶é€ ä¸šPMI'
            )
            
            return {
                "success": True,
                "data": df.to_dict('records'),
                "timestamp": datetime.now().isoformat(),
                "source": "tushare_pro"
            }
        except Exception as e:
            return self._handle_error(e, "pmi_data")
    
    def get_ppi_data(self, start_date: str, end_date: str) -> Dict:
        """è·å–PPIæ•°æ®"""
        try:
            df = self.pro_api.eco_cal(
                start_date=start_date,
                end_date=end_date,
                event='PPIåŒæ¯”'
            )
            
            return {
                "success": True,
                "data": df.to_dict('records'),
                "timestamp": datetime.now().isoformat(),
                "source": "tushare_pro"
            }
        except Exception as e:
            return self._handle_error(e, "ppi_data")
```

### 2. èšåˆæ•°æ®API
**æ•°æ®ç±»å‹**: å¤©æ°”å’ŒèŠ‚å‡æ—¥ä¿¡æ¯  
**æ›´æ–°é¢‘ç‡**: å®æ—¶/å¹´åº¦  
**ä¸»è¦æ•°æ®**:

| æ•°æ®é¡¹ | APIæ¥å£ | æ›´æ–°é¢‘ç‡ | ç”¨é€” |
|--------|---------|----------|------|
| å¤©æ°”é¢„æŠ¥ | `/weather/forecast` | å®æ—¶ | éœ€æ±‚å½±å“åˆ†æ |
| å†å²å¤©æ°” | `/weather/history` | æ—¥åº¦ | å­£èŠ‚æ€§åˆ†æ |
| èŠ‚å‡æ—¥å®‰æ’ | `/holiday/list` | å¹´åº¦ | æ¶ˆè´¹é«˜å³°é¢„æµ‹ |

```python
class JuHeDataAdapter:
    """èšåˆæ•°æ®é€‚é…å™¨"""
    
    def get_weather_forecast(self, city: str, days: int = 7) -> Dict:
        """è·å–å¤©æ°”é¢„æŠ¥"""
        try:
            response = requests.get(
                f"{self.base_url}/weather/forecast",
                params={
                    "cityname": city,
                    "days": days,
                    "key": self.api_key
                }
            )
            
            data = response.json()
            
            return {
                "success": True,
                "data": data.get("result", {}),
                "timestamp": datetime.now().isoformat(),
                "source": "juhe_api"
            }
        except Exception as e:
            return self._handle_error(e, "weather_forecast")
    
    def get_holiday_list(self, year: int) -> Dict:
        """è·å–èŠ‚å‡æ—¥åˆ—è¡¨"""
        try:
            response = requests.get(
                f"{self.base_url}/holiday/list",
                params={
                    "year": year,
                    "key": self.api_key
                }
            )
            
            data = response.json()
            
            return {
                "success": True,
                "data": data.get("result", {}),
                "timestamp": datetime.now().isoformat(),
                "source": "juhe_api"
            }
        except Exception as e:
            return self._handle_error(e, "holiday_list")
```

### 3. Google News API
**æ•°æ®ç±»å‹**: å®æ—¶æ–°é—»èµ„è®¯  
**æ›´æ–°é¢‘ç‡**: å®æ—¶  
**ä¸»è¦åŠŸèƒ½**: åˆ¶é€ ä¸šç›¸å…³æ–°é—»ç›‘æ§

```python
class GoogleNewsAdapter:
    """Googleæ–°é—»æ•°æ®é€‚é…å™¨"""
    
    def get_manufacturing_news(self, 
                             keywords: str, 
                             language: str = "zh",
                             max_results: int = 20) -> Dict:
        """è·å–åˆ¶é€ ä¸šæ–°é—»"""
        try:
            from pygooglenews import GoogleNews
            
            gn = GoogleNews(lang=language, country='CN')
            search_result = gn.search(keywords)
            
            # è§£ææ–°é—»æ•°æ®
            news_items = []
            for item in search_result['entries'][:max_results]:
                news_items.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "published": item.get("published", ""),
                    "summary": item.get("summary", ""),
                    "source": item.get("source", {}).get("title", "")
                })
            
            return {
                "success": True,
                "data": news_items,
                "timestamp": datetime.now().isoformat(),
                "source": "google_news"
            }
        except Exception as e:
            return self._handle_error(e, "manufacturing_news")
```

## ğŸ›¡ï¸ ä¸‰å±‚ç¼“å­˜æ¶æ„

### ç¼“å­˜å±‚æ¬¡è®¾è®¡
```mermaid
graph LR
    API[å¤–éƒ¨APIè°ƒç”¨] --> CACHE_CHECK{æ£€æŸ¥ç¼“å­˜}
    
    CACHE_CHECK -->|å‘½ä¸­| L1[L1: Redisç¼“å­˜]
    CACHE_CHECK -->|æœªå‘½ä¸­| L2_CHECK{æ£€æŸ¥L2}
    
    L2_CHECK -->|å‘½ä¸­| L2[L2: MongoDB]
    L2_CHECK -->|æœªå‘½ä¸­| L3_CHECK{æ£€æŸ¥L3}
    
    L3_CHECK -->|å‘½ä¸­| L3[L3: æ–‡ä»¶ç¼“å­˜]
    L3_CHECK -->|æœªå‘½ä¸­| FRESH_CALL[æ–°é²œAPIè°ƒç”¨]
    
    FRESH_CALL --> UPDATE_CACHE[æ›´æ–°æ‰€æœ‰ç¼“å­˜å±‚]
    L3 --> UPDATE_L2[æ›´æ–°L1,L2]
    L2 --> UPDATE_L1[æ›´æ–°L1]
    
    L1 --> RETURN[è¿”å›æ•°æ®]
    UPDATE_L1 --> RETURN
    UPDATE_L2 --> RETURN
    UPDATE_CACHE --> RETURN
```

### ç¼“å­˜ç­–ç•¥å®ç°
```python
class CacheManager:
    """ä¸‰å±‚ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.redis_client = redis.Redis(...)
        self.mongo_client = pymongo.MongoClient(...)
        self.file_cache_path = "./cache"
    
    def get_cached_data(self, cache_key: str) -> Optional[Dict]:
        """æŒ‰å±‚æ¬¡è·å–ç¼“å­˜æ•°æ®"""
        
        # L1: Rediså†…å­˜ç¼“å­˜
        l1_data = self._get_from_redis(cache_key)
        if l1_data:
            self._record_cache_hit("L1", cache_key)
            return l1_data
        
        # L2: MongoDBæŒä¹…åŒ–
        l2_data = self._get_from_mongodb(cache_key)
        if l2_data:
            self._record_cache_hit("L2", cache_key)
            # å›å¡«L1ç¼“å­˜
            self._set_to_redis(cache_key, l2_data, ttl=3600)
            return l2_data
        
        # L3: æ–‡ä»¶ç³»ç»Ÿç¼“å­˜
        l3_data = self._get_from_file(cache_key)
        if l3_data:
            self._record_cache_hit("L3", cache_key)
            # å›å¡«L1, L2ç¼“å­˜
            self._set_to_redis(cache_key, l3_data, ttl=3600)
            self._set_to_mongodb(cache_key, l3_data)
            return l3_data
        
        self._record_cache_miss(cache_key)
        return None
    
    def set_cached_data(self, cache_key: str, data: Dict):
        """è®¾ç½®ç¼“å­˜æ•°æ®åˆ°æ‰€æœ‰å±‚æ¬¡"""
        
        # åŒæ—¶æ›´æ–°æ‰€æœ‰ç¼“å­˜å±‚
        self._set_to_redis(cache_key, data, ttl=3600)  # 1å°æ—¶
        self._set_to_mongodb(cache_key, data)  # 7å¤©TTL
        self._set_to_file(cache_key, data)  # æ°¸ä¹…å­˜å‚¨
    
    def _get_from_redis(self, key: str) -> Optional[Dict]:
        """ä»Redisè·å–æ•°æ®"""
        try:
            data = self.redis_client.get(key)
            return json.loads(data) if data else None
        except Exception as e:
            logger.warning(f"Redisç¼“å­˜è·å–å¤±è´¥: {e}")
            return None
    
    def _get_from_mongodb(self, key: str) -> Optional[Dict]:
        """ä»MongoDBè·å–æ•°æ®"""
        try:
            collection = self.mongo_client.cache.manufacturing_data
            doc = collection.find_one({"_id": key})
            
            if doc and doc.get("expires_at") > datetime.now():
                return doc.get("data")
            return None
        except Exception as e:
            logger.warning(f"MongoDBç¼“å­˜è·å–å¤±è´¥: {e}")
            return None
```

## ğŸ”„ æ•°æ®å¤„ç†æµç¨‹

### ç«¯åˆ°ç«¯æ•°æ®æµ
```mermaid
sequenceDiagram
    participant Agent as æ™ºèƒ½ä½“
    participant Tool as æ•°æ®å·¥å…·
    participant Cache as ç¼“å­˜ç®¡ç†å™¨
    participant Adapter as æ•°æ®é€‚é…å™¨
    participant API as å¤–éƒ¨API
    participant Validator as æ•°æ®éªŒè¯å™¨

    Agent->>Tool: 1. è¯·æ±‚æ•°æ®
    Tool->>Cache: 2. æ£€æŸ¥ç¼“å­˜
    
    alt ç¼“å­˜å‘½ä¸­
        Cache-->>Tool: 3a. è¿”å›ç¼“å­˜æ•°æ®
    else ç¼“å­˜æœªå‘½ä¸­
        Cache->>Adapter: 3b. è°ƒç”¨é€‚é…å™¨
        Adapter->>API: 4. å¤–éƒ¨APIè°ƒç”¨
        API-->>Adapter: 5. è¿”å›åŸå§‹æ•°æ®
        Adapter->>Validator: 6. æ•°æ®éªŒè¯
        Validator-->>Adapter: 7. éªŒè¯ç»“æœ
        Adapter-->>Cache: 8. è¿”å›å¤„ç†åæ•°æ®
        Cache->>Cache: 9. æ›´æ–°ç¼“å­˜
        Cache-->>Tool: 10. è¿”å›æ•°æ®
    end
    
    Tool->>Tool: 11. æ•°æ®åå¤„ç†
    Tool-->>Agent: 12. è¿”å›æœ€ç»ˆæ•°æ®
```

### æ•°æ®éªŒè¯æœºåˆ¶
```python
class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    def validate_pmi_data(self, data: Dict) -> Tuple[bool, List[str]]:
        """éªŒè¯PMIæ•°æ®"""
        errors = []
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ["date", "value", "previous_value"]
        for field in required_fields:
            if field not in data:
                errors.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        if "value" in data:
            value = data["value"]
            if not (0 <= value <= 100):
                errors.append(f"PMIå€¼è¶…å‡ºæœ‰æ•ˆèŒƒå›´[0,100]: {value}")
        
        # æ£€æŸ¥æ—¶é—´æœ‰æ•ˆæ€§
        if "date" in data:
            try:
                datetime.strptime(data["date"], "%Y-%m-%d")
            except ValueError:
                errors.append(f"æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {data['date']}")
        
        return len(errors) == 0, errors
    
    def validate_weather_data(self, data: Dict) -> Tuple[bool, List[str]]:
        """éªŒè¯å¤©æ°”æ•°æ®"""
        errors = []
        
        # æ£€æŸ¥æ¸©åº¦èŒƒå›´
        if "temperature" in data:
            temp = data["temperature"]
            if not (-50 <= temp <= 60):
                errors.append(f"æ¸©åº¦è¶…å‡ºåˆç†èŒƒå›´: {temp}Â°C")
        
        # æ£€æŸ¥æ¹¿åº¦èŒƒå›´
        if "humidity" in data:
            humidity = data["humidity"]
            if not (0 <= humidity <= 100):
                errors.append(f"æ¹¿åº¦è¶…å‡ºæœ‰æ•ˆèŒƒå›´: {humidity}%")
        
        return len(errors) == 0, errors
```

## ğŸ“ˆ æ•°æ®è´¨é‡ç›‘æ§

### æ•°æ®è´¨é‡æŒ‡æ ‡
```python
class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "api_success_rate": 0.0,
            "data_completeness": 0.0,
            "data_freshness": 0.0,
            "cache_hit_rate": 0.0
        }
    
    def calculate_api_success_rate(self) -> float:
        """è®¡ç®—APIæˆåŠŸç‡"""
        total_calls = self._get_total_api_calls()
        successful_calls = self._get_successful_api_calls()
        return successful_calls / max(total_calls, 1)
    
    def calculate_data_completeness(self) -> float:
        """è®¡ç®—æ•°æ®å®Œæ•´æ€§"""
        expected_fields = self._get_expected_field_count()
        actual_fields = self._get_actual_field_count()
        return actual_fields / max(expected_fields, 1)
    
    def calculate_data_freshness(self) -> float:
        """è®¡ç®—æ•°æ®æ–°é²œåº¦"""
        current_time = datetime.now()
        data_timestamps = self._get_data_timestamps()
        
        if not data_timestamps:
            return 0.0
        
        avg_age = sum(
            (current_time - ts).total_seconds() 
            for ts in data_timestamps
        ) / len(data_timestamps)
        
        # è½¬æ¢ä¸ºæ–°é²œåº¦è¯„åˆ† (24å°æ—¶å†…ä¸º1.0ï¼Œçº¿æ€§è¡°å‡)
        return max(0.0, 1.0 - (avg_age / 86400))  # 86400ç§’ = 24å°æ—¶
```

### å¼‚å¸¸æ£€æµ‹ä¸å¤„ç†
```python
class AnomalyDetector:
    """æ•°æ®å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def detect_pmi_anomaly(self, current_value: float, historical_values: List[float]) -> bool:
        """æ£€æµ‹PMIæ•°æ®å¼‚å¸¸"""
        if len(historical_values) < 3:
            return False
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mean = statistics.mean(historical_values)
        std_dev = statistics.stdev(historical_values)
        
        # 3-sigmaè§„åˆ™æ£€æµ‹å¼‚å¸¸
        z_score = abs(current_value - mean) / max(std_dev, 0.1)
        return z_score > 3.0
    
    def detect_data_staleness(self, data_timestamp: datetime, threshold_hours: int = 24) -> bool:
        """æ£€æµ‹æ•°æ®è¿‡æœŸ"""
        current_time = datetime.now()
        age_hours = (current_time - data_timestamp).total_seconds() / 3600
        return age_hours > threshold_hours
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å¹¶å‘æ•°æ®è·å–
```python
import asyncio
import aiohttp

class ConcurrentDataFetcher:
    """å¹¶å‘æ•°æ®è·å–å™¨"""
    
    async def fetch_all_data(self, data_requests: List[Dict]) -> Dict:
        """å¹¶å‘è·å–æ‰€æœ‰æ•°æ®"""
        tasks = []
        
        for request in data_requests:
            task = self._create_fetch_task(request)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´ç†ç»“æœ
        return self._process_concurrent_results(results, data_requests)
    
    async def _create_fetch_task(self, request: Dict):
        """åˆ›å»ºå•ä¸ªè·å–ä»»åŠ¡"""
        data_type = request["type"]
        params = request["params"]
        
        if data_type == "pmi":
            return await self._fetch_pmi_data(**params)
        elif data_type == "weather":
            return await self._fetch_weather_data(**params)
        # ... å…¶ä»–æ•°æ®ç±»å‹
```

### 2. æ™ºèƒ½é¢„å–ç­–ç•¥
```python
class DataPrefetcher:
    """æ•°æ®é¢„å–å™¨"""
    
    def predict_data_needs(self, analysis_patterns: List[str]) -> List[str]:
        """é¢„æµ‹æ•°æ®éœ€æ±‚"""
        predicted_needs = []
        
        # åŸºäºå†å²åˆ†ææ¨¡å¼é¢„æµ‹
        for pattern in analysis_patterns:
            if "market_environment" in pattern:
                predicted_needs.extend(["pmi_data", "ppi_data"])
            if "trend_prediction" in pattern:
                predicted_needs.extend(["weather_data", "holiday_data"])
        
        return list(set(predicted_needs))  # å»é‡
    
    def prefetch_data(self, predicted_needs: List[str]):
        """é¢„å–æ•°æ®"""
        for data_type in predicted_needs:
            if not self._is_cached(data_type):
                self._schedule_prefetch(data_type)
```

## ğŸ“Š ç›‘æ§ä¸å‘Šè­¦

### æ€§èƒ½æŒ‡æ ‡ç›‘æ§
- **APIå“åº”æ—¶é—´**: å¹³å‡<2ç§’ï¼ŒP95<5ç§’
- **ç¼“å­˜å‘½ä¸­ç‡**: L1>80%, L2>60%, L3>40%
- **æ•°æ®æ–°é²œåº¦**: >95%æ•°æ®åœ¨24å°æ—¶å†…
- **APIæˆåŠŸç‡**: >99%

### å‘Šè­¦æœºåˆ¶
```python
class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def check_data_quality_alerts(self):
        """æ£€æŸ¥æ•°æ®è´¨é‡å‘Šè­¦"""
        
        # APIæˆåŠŸç‡å‘Šè­¦
        if self.monitor.get_api_success_rate() < 0.95:
            self._send_alert("APIæˆåŠŸç‡è¿‡ä½", "critical")
        
        # ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦
        if self.monitor.get_cache_hit_rate() < 0.7:
            self._send_alert("ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½", "warning")
        
        # æ•°æ®æ–°é²œåº¦å‘Šè­¦
        if self.monitor.get_data_freshness() < 0.9:
            self._send_alert("æ•°æ®ä¸å¤Ÿæ–°é²œ", "warning")
```

---

é€šè¿‡è¿™ç§ç²¾å¿ƒè®¾è®¡çš„æ•°æ®æµæ¶æ„ï¼Œåˆ¶é€ ä¸šæ™ºèƒ½è¡¥è´§å†³ç­–ç³»ç»Ÿç¡®ä¿äº†æ•°æ®çš„é«˜è´¨é‡ã€é«˜å¯ç”¨æ€§å’Œé«˜æ€§èƒ½ï¼Œä¸ºæ™ºèƒ½ä½“å›¢é˜Ÿæä¾›äº†å¯é çš„æ•°æ®åŸºç¡€ï¼Œæ”¯æ’‘æ•´ä¸ªå†³ç­–åˆ†ææµç¨‹çš„é¡ºåˆ©è¿è¡Œã€‚
